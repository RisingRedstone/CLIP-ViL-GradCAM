{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_A4lRslt2Anm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "refSub = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IFkchm5W2Ano"
   },
   "outputs": [],
   "source": [
    "\n",
    "def interpret_vit(image, text, model, device, index=None):\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    print(logits_per_image)\n",
    "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "    if index is None:\n",
    "        index = np.argmax(logits_per_image.cpu().data.numpy(), axis=-1)\n",
    "    one_hot = np.zeros((1, logits_per_image.size()[-1]), dtype=np.float32)\n",
    "    one_hot[0, index] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    one_hot = torch.sum(one_hot.cpu() * logits_per_image.cpu())\n",
    "    model.zero_grad()\n",
    "    one_hot.backward(retain_graph=True)\n",
    "\n",
    "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
    "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    for blk in image_attn_blocks:\n",
    "        grad = blk.attn_grad\n",
    "        cam = blk.attn_probs\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.clamp(min=0).mean(dim=0)\n",
    "        R += torch.matmul(cam, R)\n",
    "    R[0, 0] = 0\n",
    "    image_relevance = R[0, 1:]\n",
    "\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cpu().data.numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    #plt.imshow(vis)\n",
    "    return vis\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eUb_Rd5G2Anv",
    "outputId": "70082359-65a1-4a3f-f2ff-979157ae4425"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "from torchray.attribution.grad_cam import grad_cam\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "st.sidebar.header('Options')\n",
    "alpha = st.sidebar.radio(\"select alpha\", [0.5, 0.7, 0.8], index=1)\n",
    "layer = st.sidebar.selectbox(\"select saliency layer\", ['layer4.2.relu'], index=0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_rn, preprocess = clip.load(\"RN50\", device=device, jit=False)\n",
    "\n",
    "def interpret_rn(image, text, model, device, index=None):   \n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
    "    image_features_new = image_features / image_features_norm\n",
    "    text_features_norm = text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features_new = text_features / text_features_norm\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features_new @ text_features_new.t()\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().detach().numpy().tolist()\n",
    "    \n",
    "    text_prediction = (text_features_new * image_features_norm)\n",
    "    image_relevance = grad_cam(model.visual, image.type(model.dtype), text_prediction, saliency_layer=layer)\n",
    "        \n",
    "#     image_relevance = grad_cam(model.visual, image.type(model.dtype), image_features, saliency_layer=layer)\n",
    "\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cpu().data.numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    plt.imshow(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fjtms-ps2Anp",
    "outputId": "5ddd9d18-4863-4d55-dd7d-1ca22cfefc5a"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6O0lYX7N2Anq"
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SlhG0ekE2Anr",
    "outputId": "4733285c-f50b-4c6b-f6b1-2bbf51f8536c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _transform.<locals>.<lambda> at 0x000001D2C7F22B90>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "\u001b[1m\u001b[95m\u001b[4mtext: table\u001b[0m\n",
      "tensor([[20.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.38 GiB is allocated by PyTorch, and 67.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b) ViT-B/32\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfont,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.15\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43minterpret_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m133\u001b[39m)\n\u001b[0;32m     49\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36minterpret_vit\u001b[1;34m(image, text, model, device, index)\u001b[0m\n\u001b[0;32m     10\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(one_hot\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m*\u001b[39m logits_per_image\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mone_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m image_attn_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mresblocks\u001b[38;5;241m.\u001b[39mnamed_children())\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     15\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m image_attn_blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn_probs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mD:\\Software\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Software\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.38 GiB is allocated by PyTorch, and 67.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "img_id = 'COCO_val2014_000000393267'\n",
    "MSCOCO_IMG_ROOT = \"/rscratch/data/coco_2014/images\"\n",
    "\n",
    "# COCO_val2014_000000393267 What color is the woman's shirt on the left? {'black': 1, 'blonde': 0.3}\n",
    "import os\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "ori_preprocess = Compose([\n",
    "        Resize((224), interpolation=Image.BICUBIC),\n",
    "    CenterCrop(size=(224, 224)),\n",
    "        ToTensor()])\n",
    "img_path = os.path.join(MSCOCO_IMG_ROOT, \"val2014\", img_id + \".jpg\")\n",
    "img_path = 'FireCam.jpg'\n",
    "image = ori_preprocess(Image.open(img_path))\n",
    "print(preprocess)\n",
    "\n",
    "from matplotlib import rc\n",
    "\n",
    "font = {\n",
    "    'size': 32,\n",
    "}\n",
    "import matplotlib\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'custom'\n",
    "matplotlib.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
    "matplotlib.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
    "matplotlib.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
    "# matplotlib.rcParams['mathtext.size'] = 16\n",
    "\n",
    "# {'cursive', 'fantasy', 'monospace', 'sans', 'sans serif', 'sans-serif', 'serif'}\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.tight_layout()\n",
    "plt.subplot(131)\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.title(\"(a) Original\", **font, y=-0.15)\n",
    "\n",
    "# plt.savefig('/rscratch/sheng.s/clip_boi/clip_vqa_starting/visual/sample_1_ori.pdf', bbox_inches='tight')\n",
    "# plt.show()\n",
    "texts = [\"table\"]\n",
    "\n",
    "\n",
    "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(texts).to(device)\n",
    "print(color.BOLD + color.PURPLE + color.UNDERLINE + 'text: ' + texts[0] + color.END)\n",
    "plt.subplot(132)\n",
    "plt.axis('off')\n",
    "plt.title(\"(b) ViT-B/32\", **font,y=-0.15)\n",
    "interpret_vit(model=model, image=image, text=text, device=device, index=0)\n",
    "plt.subplot(133)\n",
    "plt.axis('off')\n",
    "plt.title(\"(c) RN50\", **font,y=-0.15)\n",
    "interpret_rn(model=model_rn, image=image, text=text, device=device, index=0)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(texts).to(device)\n",
    "plt.subplot(133)\n",
    "print(color.BOLD + color.PURPLE + color.UNDERLINE + 'text: ' + texts[0] + color.END)\n",
    "interpret_rn(model=model_rn, image=image, text=text, device=device, index=0)\n",
    "plt.axis('off')\n",
    "plt.title(\"(c) RN50\", **font,y=-0.15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('sample_all.pdf', bbox_inches='tight')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Where are the buildings?\"]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "vid_path = \"vids\"\n",
    "save_path = \"out\"\n",
    "count = 0\n",
    "refSub = None\n",
    "img_path = lambda a, b: a + \"\\\\\" + str(b) + \".jpg\" \n",
    "img = Image.open(img_path(vid_path, count))\n",
    "while(True):\n",
    "    image = preprocess(img).unsqueeze(0).to(device)\n",
    "    img_ret = interpret_vit(model=model, image=image, text=text, device=device, index=0)\n",
    "    Image.fromarray(img_ret).save(img_path(save_path, count))\n",
    "    img = None\n",
    "    try:\n",
    "        count+=1\n",
    "        img = Image.open(img_path(vid_path, count))\n",
    "    except:\n",
    "        print(\"File Ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "from glob import glob\n",
    "\n",
    "image_files = glob('bottle/*.jpg')\n",
    "\n",
    "# Load and store images in a list\n",
    "images = [preprocess(Image.open(file)).unsqueeze(0).cpu() for file in image_files]\n",
    "images = np.stack(images)\n",
    "images = torch.Tensor(images).squeeze(1).to(device)\n",
    "texts= [\"bottle\", \"mom\", \"lion\"]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "logits_per_image, logits_per_text = model(images, text)\n",
    "probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "gradcam_clip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
