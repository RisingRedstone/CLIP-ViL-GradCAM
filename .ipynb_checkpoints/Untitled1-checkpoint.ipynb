{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167db373-c248-405f-9f3b-6230eb217814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1467a28-a74c-4a59-961b-0a59e363156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "refSub = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adfe612d-2c5b-4546-818b-d0cb7a5f7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "RelevMin = None\n",
    "RelevMax = None\n",
    "def interpret_vit(image, text, model, device, index=None, dangerVal = 10):\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    print(logits_per_image)\n",
    "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "    if index is None:\n",
    "        index = np.argmax(logits_per_image.cpu().data.numpy(), axis=-1)\n",
    "    one_hot = np.zeros((1, logits_per_image.size()[-1]), dtype=np.float32)\n",
    "    one_hot[0, index] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    one_hot = torch.sum(one_hot.cpu() * logits_per_image.cpu())\n",
    "    model.zero_grad()\n",
    "    one_hot.backward(retain_graph=False)\n",
    "\n",
    "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
    "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    for blk in image_attn_blocks:\n",
    "        grad = blk.attn_grad\n",
    "        cam = blk.attn_probs\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.clamp(min=0).mean(dim=0)\n",
    "        R += torch.matmul(cam, R)\n",
    "    R[0, 0] = 0\n",
    "    image_relevance = R[0, 1:]\n",
    "\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    alpha = 0.1\n",
    "    global RelevMin\n",
    "    global RelevMax\n",
    "    if(RelevMax == None):\n",
    "        RelevMax = image_relevance.max()\n",
    "        RelevMin = image_relevance.min()\n",
    "    else:\n",
    "        RelevMax = (1.0-alpha) * RelevMax + alpha * torch.maximum(image_relevance.max(), RelevMax)\n",
    "        RelevMin = (1.0-alpha) * RelevMin + alpha * torch.minimum(image_relevance.min(), RelevMin)\n",
    "\n",
    "    #CurrRelevMax = torch.maximum(RelevMax, image_relevance.max()).cpu().detach().numpy()\n",
    "    #CurrRelevMin = torch.minimum(RelevMin, image_relevance.min()).cpu().detach().numpy()\n",
    "    \n",
    "    #CurrRelevMax = RelevMax.cpu().detach().numpy()\n",
    "    #CurrRelevMin = RelevMin.cpu().detach().numpy()\n",
    "    \n",
    "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cpu().data.numpy()\n",
    "    CurrRelevMax = image_relevance.max()\n",
    "    CurrRelevMin = image_relevance.min()\n",
    "    image_relevance = (image_relevance - CurrRelevMin) / (CurrRelevMax - CurrRelevMin)\n",
    "    image_relevance = np.maximum(0.0, np.minimum(1.0, image_relevance))\n",
    "    image_relevance *= dangerVal/10\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    \n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    #vis = image_relevance\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "\n",
    "    #plt.imshow(vis)\n",
    "    return vis\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ecb356b-4207-451c-ad50-bd601fc93508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n"
     ]
    }
   ],
   "source": [
    "vid_path = '5.mp4'\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "return_path = \"vids\"\n",
    "count = 0\n",
    "trCount = 0\n",
    "\n",
    "OriginalSize = None\n",
    "while(cap.isOpened() == True):\n",
    "    ret, frame = cap.read()\n",
    "    if(frame is not None):\n",
    "        OriginalSize = frame.shape\n",
    "    if ret == True:\n",
    "        if(count%1 == 0):\n",
    "            cv2.imwrite(return_path + \"\\\\\" + str(trCount) + \".jpg\", cv2.resize(frame, (OriginalSize[0], OriginalSize[0])))\n",
    "            trCount += 1\n",
    "            print(trCount)\n",
    "        count+=1\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "          break\n",
    "    \n",
    "    # Break the loop\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "922b0a1c-da46-406a-baf0-819671c5ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083b20d7-f1f2-4014-91b7-e76d532c6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86660115-5e47-4d04-bf00-bd3413d3e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dangerMag(image):\n",
    "    texts = [\"Is there violence?\", \n",
    "            \"Is there fire?\",\n",
    "            \"Is someone sleeping?\",\n",
    "            \"Is someone walking?\"]\n",
    "    dangerValues = [10, 7, 0, 0]\n",
    "    dangerTensor = torch.tensor([dangerValues]).float().to(device)\n",
    "    text = clip.tokenize(texts).to(device)\n",
    "    enc_text = model.encode_text(text)\n",
    "    enc_img = model.encode_image(image)\n",
    "    logits = torch.mm(enc_img, torch.transpose(enc_text, 0, 1))\n",
    "    probs = torch.nn.Softmax(dim = 1)(logits)\n",
    "    danger = torch.tensordot(dangerTensor.float(), probs.float(), dims = 2)\n",
    "    print(probs.cpu().detach().numpy(), end = \" \")\n",
    "    return danger.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7067c1e5-0e00-4fea-90c5-085f4bbb76da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.398   0.1176  0.04065 0.4438 ]] tensor([[18.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.516   0.1305  0.03052 0.323  ]] tensor([[18.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5874  0.09    0.02277 0.3    ]] tensor([[18.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.607   0.06104 0.03168 0.3005 ]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5923  0.05176 0.0286  0.3271 ]] tensor([[18.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.568   0.06174 0.02068 0.3499 ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.595   0.0627  0.01883 0.3235 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5386  0.0764  0.02025 0.3645 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.481   0.0849  0.02284 0.4114 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4502  0.0619  0.02348 0.4644 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3955  0.072   0.02489 0.508  ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6396  0.0718  0.00884 0.2795 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5327  0.0926  0.01398 0.3606 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.543   0.0712  0.01802 0.3674 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5146 0.0789 0.0237 0.3826]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4976  0.07874 0.03035 0.3936 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5615 0.065  0.0332 0.3406]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.637   0.06213 0.03075 0.2698 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.585   0.05792 0.02911 0.3281 ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6606  0.0464  0.02602 0.2668 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.629   0.05493 0.024   0.2925 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6255  0.04825 0.02107 0.305  ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.584   0.04297 0.01906 0.354  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6445  0.0393  0.01665 0.2996 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.611   0.04156 0.01553 0.332  ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6323  0.0417  0.01312 0.313  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.709   0.0547  0.01319 0.2231 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6807  0.06134 0.01171 0.2465 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6997  0.05835 0.01453 0.2272 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.728   0.05112 0.01233 0.2086 ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6416  0.0645  0.01395 0.2803 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5835  0.07306 0.01605 0.3274 ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6187  0.06128 0.01346 0.3064 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.687    0.04675  0.009796 0.2566  ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6943  0.04877 0.00902 0.2477 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7573  0.04407 0.00731 0.1914 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6963  0.04053 0.01091 0.2522 ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.637   0.04404 0.01302 0.3057 ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6704   0.03235  0.008575 0.2883  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6353  0.0475  0.01258 0.3047 ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6377  0.04477 0.01168 0.306  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6455  0.05643 0.01146 0.2864 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.652    0.06158  0.010376 0.2761  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.668    0.04547  0.008026 0.2786  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7183   0.0336   0.007614 0.2406  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.716   0.04102 0.00735 0.236  ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6885   0.0476   0.006443 0.2573  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.65     0.04636  0.005985 0.2976  ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.689   0.03123 0.00543 0.2742 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7065  0.03253 0.00531 0.2559 ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7026   0.05334  0.004963 0.239   ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7524   0.0562   0.004204 0.1873  ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7393   0.04953  0.005917 0.2053  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7583  0.0323  0.00519 0.2041 ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.646    0.0556   0.006958 0.2913  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5522   0.06003  0.008385 0.3794  ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5747   0.05783  0.007465 0.3599  ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.508    0.0803   0.010376 0.4016  ]] tensor([[18.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5273  0.0888  0.01028 0.3738 ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5703  0.08887 0.01045 0.33   ]] tensor([[18.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.587   0.1211  0.01074 0.2815 ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6157  0.1332  0.00996 0.2411 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6074  0.10724 0.01131 0.274  ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4055  0.1718  0.01727 0.4055 ]] tensor([[18.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3677 0.0944 0.0192 0.5186]] tensor([[18.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3916  0.1375  0.02043 0.4507 ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5933   0.08954  0.014175 0.303   ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.607    0.0847   0.012596 0.2957  ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6416  0.05267 0.01194 0.2937 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5137  0.05762 0.01625 0.4126 ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.517   0.05118 0.01636 0.4155 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.556   0.05176 0.01553 0.3765 ]] tensor([[18.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6055  0.0393  0.01588 0.3396 ]] tensor([[18.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5317  0.05182 0.02127 0.3953 ]] tensor([[18.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.49    0.0541  0.03035 0.4255 ]] tensor([[18.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4697 0.0676 0.0351 0.4275]] tensor([[18.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.363   0.1107  0.04544 0.481  ]] tensor([[17.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3816  0.1318  0.03336 0.4531 ]] tensor([[17.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3958  0.1065  0.03513 0.4626 ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.37   0.124  0.0304 0.4753]] tensor([[18.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4548  0.0731  0.02411 0.4478 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5493  0.04874 0.01878 0.3833 ]] tensor([[18.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.601   0.0509  0.01602 0.332  ]] tensor([[18.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6455  0.04532 0.0134  0.2957 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6704  0.04492 0.0137  0.271  ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.671   0.04565 0.01628 0.2668 ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6255  0.05637 0.0183  0.3    ]] tensor([[18.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6084  0.0515  0.01956 0.3206 ]] tensor([[18.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6123  0.05695 0.0182  0.3127 ]] tensor([[18.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6265  0.04468 0.01892 0.31   ]] tensor([[17.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6494  0.0456  0.02597 0.2793 ]] tensor([[17.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6772  0.06012 0.0211  0.2415 ]] tensor([[18.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.682   0.0569  0.01762 0.2433 ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5986  0.07153 0.01895 0.3108 ]] tensor([[18.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.534   0.059   0.02238 0.3845 ]] tensor([[18.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.606   0.04456 0.01517 0.3345 ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5254  0.04114 0.02457 0.409  ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.622   0.04166 0.01878 0.3176 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.665   0.0513  0.01492 0.2688 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5327  0.05795 0.02531 0.3838 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6177  0.05313 0.01894 0.3105 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6826  0.041   0.01351 0.2632 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6377  0.04996 0.01572 0.2966 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3872  0.07623 0.03177 0.505  ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3745 0.1073 0.0298 0.4885]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5015  0.0858  0.02829 0.3845 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6123  0.0882  0.02338 0.2761 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.649   0.0787  0.01456 0.258  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7256  0.0707  0.01173 0.1923 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6816  0.1062  0.01068 0.2015 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7446   0.0889   0.007652 0.1586  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7344  0.0603  0.01064 0.1946 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7236  0.06525 0.00985 0.201  ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7295  0.09564 0.00977 0.1653 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.639   0.1747  0.01152 0.1747 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.659   0.1888  0.01389 0.1382 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7065  0.11536 0.01274 0.1653 ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6733  0.187   0.00916 0.1305 ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6743   0.1328   0.011246 0.1815  ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.575   0.2673  0.00747 0.15   ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6597  0.2043  0.00843 0.1278 ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.571    0.3254   0.007534 0.0962  ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5933  0.3381  0.00419 0.0645 ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6055 0.2905 0.0054 0.0988]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6074  0.2612  0.00624 0.1254 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6523   0.165    0.009605 0.1729  ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7534  0.08453 0.01109 0.1508 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7773  0.08325 0.0085  0.131  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.782   0.0642  0.00883 0.1447 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7383  0.05783 0.01121 0.1926 ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.745  0.0704 0.0103 0.1742]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8564  0.05304 0.00595 0.0848 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8438   0.06018  0.005775 0.09033 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8022   0.0723   0.007744 0.11743 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.754   0.07465 0.00864 0.163  ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.749    0.079    0.007458 0.1646  ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7646   0.0658   0.006824 0.1628  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7656  0.06186 0.00716 0.1655 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7275   0.072    0.007477 0.1929  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.609   0.0979  0.00984 0.2832 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.618   0.1143  0.01015 0.2576 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5117  0.2036  0.01079 0.274  ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.618    0.1217   0.010635 0.2496  ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.787   0.0804  0.00821 0.1245 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8193  0.06726 0.00606 0.1075 ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.778   0.09    0.00934 0.12305]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.827    0.07117  0.007156 0.0943  ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.869   0.0353  0.00409 0.0916 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.817    0.063    0.004288 0.11584 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8174   0.06305  0.003784 0.1159  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8125  0.07214 0.00348 0.1117 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8535  0.07227 0.00303 0.07117]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.804   0.0902  0.00517 0.10065]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8374   0.0616   0.003878 0.0969  ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7534  0.0734  0.00774 0.1655 ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7944  0.05844 0.00925 0.1381 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8447  0.0414  0.00828 0.1057 ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7715  0.0525  0.01171 0.1643 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.67    0.0533  0.01842 0.2583 ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6626  0.057   0.01659 0.2637 ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6826   0.04947  0.012505 0.2551  ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6616  0.0625  0.01683 0.259  ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.741   0.04248 0.01358 0.2026 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.663   0.07794 0.0191  0.2401 ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7812   0.06122  0.010475 0.1469  ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.715   0.06055 0.01656 0.208  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7495  0.0391  0.00989 0.2017 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7773   0.02579  0.006416 0.1904  ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.728    0.02452  0.007595 0.24    ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.746   0.02554 0.00755 0.2206 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.731    0.03064  0.008644 0.23    ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.769    0.0178   0.005966 0.207   ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.744   0.01544 0.00624 0.2341 ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7124  0.01784 0.00767 0.262  ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.639   0.02597 0.00843 0.3264 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.625   0.02539 0.0101  0.3396 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6523  0.02611 0.00835 0.313  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.596   0.025   0.01127 0.3674 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6387   0.0272   0.007793 0.3262  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6387  0.03897 0.00626 0.3162 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5635  0.04144 0.00816 0.3872 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.47    0.07916 0.0096  0.4414 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4192  0.0674  0.01575 0.4978 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.522   0.04285 0.016   0.4194 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.564   0.04086 0.01347 0.3816 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7275  0.02267 0.00975 0.24   ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.84    0.01316 0.0075  0.1393 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.85    0.01533 0.00846 0.1263 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8354  0.02028 0.01192 0.1322 ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8306   0.02507  0.013214 0.1313  ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.844    0.01836  0.011856 0.1255  ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.825    0.02234  0.013336 0.139   ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8145  0.02745 0.0118  0.1461 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.843   0.02884 0.00866 0.11957]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.791   0.0439  0.01201 0.1533 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7705  0.04926 0.01368 0.1666 ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8003  0.0411  0.01314 0.1458 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8027  0.03366 0.01042 0.1532 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7993  0.03857 0.01659 0.1455 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.81    0.03503 0.01437 0.1407 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8564  0.01892 0.00867 0.1159 ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.82     0.02084  0.007668 0.1516  ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8354   0.02296  0.005287 0.1364  ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.865    0.01714  0.004265 0.11346 ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.856    0.01921  0.006958 0.1177  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.872   0.01674 0.00882 0.10254]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.829    0.014046 0.01044  0.1464  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7837  0.01901 0.01135 0.1862 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.663   0.02234 0.01072 0.3037 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.618    0.02802  0.012436 0.3413  ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.744   0.02841 0.01078 0.2166 ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.755   0.02927 0.00965 0.2063 ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7764  0.03107 0.0081  0.1844 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7153  0.0356  0.00915 0.2396 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7144  0.03613 0.01019 0.2393 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.805   0.02628 0.00777 0.161  ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.798   0.02061 0.009   0.1725 ]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8076   0.0222   0.010994 0.159   ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.839   0.02495 0.00932 0.1267 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6943  0.03857 0.01158 0.2554 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6304  0.04028 0.01248 0.317  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5527  0.03534 0.01342 0.3982 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.555   0.0344  0.01082 0.3997 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7153  0.02953 0.00783 0.2472 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8086  0.02403 0.00818 0.1592 ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7495  0.03397 0.01174 0.2048 ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.661   0.04428 0.01461 0.28   ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7183  0.04114 0.01125 0.2295 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7417  0.0375  0.00824 0.2125 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.799    0.03146  0.007244 0.1624  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7573  0.03543 0.00968 0.1975 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6787  0.03275 0.01015 0.2786 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.614    0.04047  0.012344 0.3335  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.653   0.0392  0.01334 0.2944 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.663   0.04514 0.01512 0.2764 ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7114  0.03656 0.01366 0.2383 ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.768   0.0312  0.00952 0.1912 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7026  0.03287 0.01018 0.2544 ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.741   0.02658 0.00963 0.2225 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7217   0.0407   0.013855 0.2236  ]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6562  0.04828 0.01749 0.2778 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.634   0.03412 0.01336 0.3186 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6934  0.0356  0.01605 0.2551 ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6616  0.03143 0.01352 0.2935 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.803   0.04257 0.01239 0.1417 ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.879   0.02916 0.01008 0.0818 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8555  0.03757 0.01259 0.0945 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.854   0.04187 0.01127 0.0929 ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.757    0.04407  0.013435 0.1855  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6377  0.06616 0.00881 0.2874 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6904  0.05078 0.00883 0.25   ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6035  0.05276 0.01007 0.3335 ]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6006  0.04633 0.01083 0.3423 ]] tensor([[19.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4937  0.04813 0.01563 0.4426 ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6826  0.02954 0.01175 0.276  ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6333  0.03253 0.021   0.3135 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.802    0.01828  0.011986 0.1681  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.778   0.01402 0.01447 0.1936 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7944  0.01869 0.01225 0.1746 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.693   0.02773 0.0163  0.263  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.684   0.0213  0.01851 0.2764 ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7256  0.01964 0.01933 0.2355 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.578   0.02704 0.02173 0.3733 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4788  0.03467 0.02274 0.4639 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.477   0.03793 0.03001 0.455  ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6514  0.02731 0.01848 0.303  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6533  0.02307 0.01942 0.304  ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.695    0.01881  0.013985 0.2722  ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.67    0.02722 0.0148  0.288  ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.606   0.02501 0.01859 0.3506 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4895  0.0318  0.01869 0.46   ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.37    0.0525  0.02188 0.5557 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4236 0.0506 0.0228 0.503 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.518    0.05048  0.015396 0.4163  ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.537   0.0484  0.01572 0.399  ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4397  0.02293 0.01528 0.522  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2578  0.02554 0.02635 0.6904 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.187   0.03516 0.06073 0.7173 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3035  0.05032 0.0424  0.6035 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.294   0.04044 0.04303 0.6226 ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2917  0.01955 0.05148 0.637  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2952  0.01918 0.0406  0.645  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4329  0.02682 0.04218 0.4983 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5283  0.028   0.03885 0.405  ]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5513  0.02878 0.0347  0.385  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5474  0.0256  0.03287 0.3943 ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6177  0.03076 0.03595 0.3154 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7246  0.02328 0.0244  0.2279 ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6704  0.04028 0.01816 0.271  ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6885  0.0317  0.01451 0.2654 ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8115  0.0245  0.01122 0.1525 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7437  0.05063 0.0145  0.191  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6455  0.0702  0.01518 0.269  ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7373  0.07416 0.01052 0.1779 ]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8647   0.02304  0.007366 0.1049  ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7627  0.03918 0.01122 0.1869 ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8184  0.04074 0.00708 0.1337 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8213  0.04633 0.00844 0.12396]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8936   0.03156  0.002844 0.0722  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.83     0.0461   0.004158 0.11957 ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.747    0.05414  0.003683 0.195   ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6367   0.05923  0.007652 0.2961  ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.726   0.02687 0.00746 0.2395 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7275   0.02376  0.008606 0.24    ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7856  0.02774 0.00859 0.1781 ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.785    0.02446  0.006584 0.1836  ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8022   0.01746  0.003775 0.1763  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7803   0.02014  0.005596 0.1942  ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8467  0.02054 0.00519 0.1278 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8374   0.0245   0.005642 0.1324  ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.861   0.01414 0.00459 0.12024]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.774    0.01657  0.004192 0.2051  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.795    0.02477  0.005703 0.1746  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8296  0.02045 0.00586 0.1442 ]] tensor([[19.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8843  0.01779 0.00457 0.0932 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.816    0.02707  0.008255 0.1486  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.915    0.0165   0.002304 0.0663  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.922    0.01715  0.002113 0.05893 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9272   0.01808  0.001604 0.05313 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9487  0.01582 0.00134 0.03403]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9478   0.01657  0.001448 0.034   ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9062   0.0325   0.003532 0.05792 ]] tensor([[17.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6816   0.0908   0.009575 0.2179  ]] tensor([[17.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6636   0.10333  0.010895 0.2223  ]] tensor([[17.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7017   0.09064  0.009705 0.1979  ]] tensor([[17.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.747    0.0717   0.006268 0.1747  ]] tensor([[17.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.902    0.02599  0.002697 0.0695  ]] tensor([[18.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9043   0.03674  0.003016 0.05603 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.893    0.02417  0.002394 0.0805  ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9146   0.014786 0.002197 0.06836 ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8584  0.01868 0.00282 0.1199 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.859    0.02592  0.002733 0.1126  ]] tensor([[18.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8896   0.01846  0.002619 0.0895  ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8555  0.02315 0.00384 0.1176 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8325  0.0294  0.00616 0.1317 ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7144   0.0367   0.013084 0.2356  ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.757    0.0354   0.007305 0.2006  ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8013   0.04382  0.006615 0.1482  ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7466  0.08246 0.00709 0.164  ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.839    0.0627   0.005833 0.09265 ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8735   0.04703  0.004375 0.07513 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8486  0.0634  0.00638 0.0814 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8506   0.0437   0.007133 0.09845 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8047  0.04333 0.00765 0.1443 ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7983  0.04718 0.00929 0.1454 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.793   0.03027 0.01282 0.1637 ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6553  0.04898 0.0183  0.2773 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.555   0.0344  0.01049 0.4    ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5747   0.02687  0.009735 0.3887  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.808    0.01391  0.005886 0.1721  ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5703  0.02353 0.00839 0.398  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.57    0.03214 0.01279 0.3855 ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.657   0.0507  0.00923 0.2827 ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.655   0.05292 0.01058 0.2817 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3906   0.06274  0.012955 0.5337  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3918  0.04984 0.01405 0.544  ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.461    0.04636  0.009865 0.483   ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.593   0.04428 0.00914 0.354  ]] tensor([[18.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.663    0.038    0.009026 0.2898  ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6987   0.03424  0.010124 0.257   ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7656   0.03262  0.004925 0.1967  ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.815    0.0311   0.003489 0.1508  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8164   0.02245  0.003084 0.1582  ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.77     0.02594  0.003456 0.2008  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.809   0.02332 0.00341 0.1643 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7     0.04544 0.00502 0.2496 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6      0.05005  0.007557 0.342   ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.713    0.0212   0.007442 0.2583  ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.761    0.02298  0.004597 0.2113  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.813    0.01973  0.004612 0.1626  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7935  0.03174 0.00569 0.169  ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.595   0.0679  0.00786 0.3289 ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5747  0.0569  0.00914 0.3596 ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6504  0.0429  0.01329 0.2932 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.628   0.05072 0.0108  0.3108 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.524   0.0646  0.02231 0.3894 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.56    0.0513  0.02106 0.3674 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5557  0.0706  0.02055 0.3533 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.682    0.0518   0.011024 0.255   ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.55    0.0688  0.01535 0.3662 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4746   0.07275  0.006985 0.4458  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6313   0.09235  0.009285 0.2673  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6094  0.08777 0.01065 0.2922 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3958  0.0677  0.01214 0.5244 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.493   0.0699  0.00889 0.4282 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.632   0.03348 0.00702 0.3276 ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.634    0.0369   0.010735 0.3186  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4338  0.03293 0.00989 0.5234 ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.462   0.03674 0.00959 0.4917 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.58    0.0507  0.01204 0.3574 ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.674   0.05197 0.01008 0.264  ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6055  0.0521  0.01297 0.3293 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4678  0.04852 0.02328 0.4604 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5513   0.03635  0.015396 0.397   ]] tensor([[17.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6694  0.0272  0.01066 0.2925 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6504   0.03043  0.011734 0.3074  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.745   0.0205  0.00708 0.2273 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6675  0.02798 0.00853 0.2961 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6123   0.015564 0.0068   0.3655  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7393   0.01685  0.007835 0.2362  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.699   0.02356 0.01184 0.2654 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7046  0.02338 0.00847 0.2634 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5684   0.01826  0.010574 0.4028  ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.688   0.01862 0.01567 0.2778 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.732   0.02751 0.01721 0.2233 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.774   0.02162 0.02063 0.1838 ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.824   0.01557 0.01486 0.1455 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.848   0.01816 0.01173 0.12213]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.789   0.03577 0.01468 0.1603 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7627  0.02821 0.01006 0.199  ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8804   0.01217  0.005486 0.1019  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.86     0.02023  0.005116 0.11456 ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.914    0.01648  0.003002 0.0662  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9062  0.02002 0.00293 0.071  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8926   0.01794  0.002626 0.087   ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.859    0.02847  0.003677 0.1092  ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8257   0.02454  0.004265 0.1458  ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8584  0.01782 0.00203 0.12177]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.792    0.03943  0.005093 0.1635  ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.73     0.0658   0.007504 0.1965  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7437  0.04465 0.00525 0.2065 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7544   0.03528  0.004147 0.2062  ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.819    0.01726  0.002527 0.1613  ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.781   0.02147 0.00345 0.1943 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8774   0.02376  0.001832 0.0969  ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8374   0.01968  0.002012 0.141   ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.749    0.0494   0.002922 0.1985  ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6943   0.04105  0.004982 0.2595  ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7314  0.02278 0.00463 0.2412 ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.754   0.03062 0.00642 0.2092 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.714    0.03964  0.011185 0.2354  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7944  0.03384 0.01016 0.1614 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.69    0.07275 0.01304 0.2241 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7124  0.0663  0.01049 0.2107 ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8667   0.0445   0.007153 0.08185 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8135  0.04446 0.00932 0.1328 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8247  0.04236 0.01252 0.12067]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.889    0.02814  0.003633 0.0789  ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.868    0.03754  0.002987 0.0915  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.805    0.02843  0.005424 0.161   ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.719    0.0347   0.005157 0.241   ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6997   0.05655  0.005775 0.238   ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.602   0.108   0.00529 0.2844 ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.787    0.094    0.003645 0.1152  ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8223   0.0765   0.004383 0.0967  ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7905   0.0724   0.006035 0.1311  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8906   0.03     0.002663 0.0766  ]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8345   0.04932  0.003517 0.1129  ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8057   0.04547  0.004295 0.1444  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.742   0.06903 0.00716 0.1819 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.697   0.0628  0.01091 0.2297 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5654 0.1594 0.0123 0.263 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.47     0.12067  0.013756 0.3958  ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5156  0.10645 0.01194 0.3657 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5723  0.08777 0.01898 0.321  ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.664   0.0899  0.01275 0.2332 ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.509   0.10834 0.01636 0.3665 ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5625 0.1295 0.0205 0.2874]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6274  0.081   0.01779 0.274  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6665  0.0821  0.01383 0.2377 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6035  0.1099  0.00991 0.2764 ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7     0.0738  0.01243 0.2135 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.657   0.09766 0.01474 0.2306 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5894  0.1197  0.01282 0.2783 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.524   0.1065  0.00916 0.36   ]] tensor([[18.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4966  0.0651  0.01365 0.4248 ]] tensor([[18.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4856  0.04807 0.0104  0.456  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5083  0.0656  0.01123 0.4148 ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4497  0.04523 0.01092 0.4941 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4814  0.0762  0.01753 0.4248 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.698   0.0426  0.01012 0.249  ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8003   0.02655  0.007725 0.1652  ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.625    0.03638  0.009056 0.3293  ]] tensor([[19.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3528 0.0344 0.0222 0.591 ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2947 0.0512 0.0301 0.624 ]] tensor([[20.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.289   0.04166 0.01764 0.6514 ]] tensor([[19.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3616  0.05902 0.01947 0.56   ]] tensor([[19.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3748  0.08105 0.01572 0.5283 ]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3142  0.0794  0.01947 0.587  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3262  0.09796 0.02086 0.555  ]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.403   0.06683 0.0207  0.5093 ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3127  0.08826 0.03247 0.5664 ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1727  0.1345  0.03049 0.662  ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2087  0.1504  0.02737 0.614  ]] tensor([[20.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3325  0.1114  0.03293 0.523  ]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2456 0.0918 0.0548 0.608 ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4253  0.1277  0.02185 0.4253 ]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5264  0.0886  0.01773 0.3674 ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4358  0.11365 0.0217  0.429  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4685 0.1498 0.0168 0.365 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.477   0.2893  0.00847 0.2253 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.438   0.1467  0.01046 0.405  ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.38    0.3513  0.01165 0.257  ]] tensor([[20.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.43    0.2489  0.01146 0.3098 ]] tensor([[20.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4873  0.1765  0.01164 0.3247 ]] tensor([[20.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4812   0.2238   0.007538 0.2874  ]] tensor([[19.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4956   0.3008   0.009514 0.1941  ]] tensor([[20.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4714  0.237   0.00979 0.2815 ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6504  0.1148  0.01003 0.2247 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4534  0.134   0.01247 0.4001 ]] tensor([[19.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.391   0.2094  0.00851 0.391  ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3623   0.1969   0.010765 0.4302  ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4424  0.0722  0.01467 0.471  ]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4248  0.0672  0.01926 0.4888 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.656   0.02623 0.01775 0.3003 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.739    0.03247  0.013535 0.2151  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7783  0.0439  0.00964 0.1683 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.783   0.0415  0.00843 0.1667 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7437  0.04465 0.01495 0.197  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8267  0.03308 0.01125 0.1288 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.707   0.0711  0.01637 0.2057 ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7812 0.0371 0.0205 0.1613]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.694   0.02568 0.0291  0.2512 ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.677   0.01694 0.02881 0.2776 ]] tensor([[19.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.777    0.010086 0.01663  0.1964  ]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.766   0.01076 0.01381 0.2095 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.757   0.0205  0.01547 0.207  ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.587   0.01947 0.0204  0.373  ]] tensor([[20.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.583   0.01602 0.01874 0.3823 ]] tensor([[20.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.706   0.0161  0.01377 0.264  ]] tensor([[20.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.602   0.0179  0.02605 0.354  ]] tensor([[19.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3572  0.02145 0.0322  0.589  ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4363  0.03418 0.03525 0.4944 ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5664  0.0291  0.02097 0.3833 ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5044  0.04077 0.01622 0.4385 ]] tensor([[20.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3518  0.1335  0.02629 0.4883 ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2903  0.06274 0.03256 0.6143 ]] tensor([[19.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3496  0.0598  0.04047 0.5503 ]] tensor([[20.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3406  0.07135 0.03534 0.5527 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.297   0.04147 0.04214 0.619  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.225  0.0367 0.0345 0.7036]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1708  0.04523 0.04248 0.7417 ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.216   0.0331  0.03162 0.719  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1559  0.02795 0.03647 0.78   ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1366 0.0362 0.0535 0.774 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08673 0.0367  0.06647 0.81   ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0921  0.03387 0.0907  0.783  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0956  0.02869 0.0756  0.8003 ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1333  0.02245 0.07715 0.767  ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.11066 0.02711 0.0822  0.7803 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2458  0.01987 0.04477 0.6895 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1376  0.02708 0.03117 0.804  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0935  0.03604 0.05005 0.8203 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1649  0.02911 0.05524 0.751  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3071  0.02481 0.02812 0.64   ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1525  0.03049 0.0666  0.7505 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1731  0.04175 0.04514 0.74   ]] tensor([[19.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1544  0.02682 0.0471  0.772  ]] tensor([[19.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.257   0.03268 0.0433  0.667  ]] tensor([[19.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1726  0.03293 0.045   0.7495 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1175  0.02313 0.03113 0.828  ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10315 0.0282  0.03146 0.8374 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.06586 0.02039 0.01915 0.8945 ]] tensor([[19.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09814 0.01137 0.02902 0.8613 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2351  0.01625 0.0355  0.713  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2627  0.02124 0.03445 0.6816 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1656  0.02042 0.03583 0.7783 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1862  0.01872 0.0471  0.748  ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0844  0.02309 0.0399  0.8525 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1715 0.0255 0.0343 0.7686]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1759  0.03357 0.03806 0.7524 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2559  0.03152 0.02783 0.6846 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2347  0.03714 0.02718 0.7007 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3418  0.02477 0.01456 0.619  ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2229  0.02747 0.01859 0.731  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.206   0.03415 0.018   0.7417 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1738  0.02838 0.01862 0.7793 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1366  0.02689 0.01311 0.823  ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.157  0.0324 0.0133 0.7974]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1217  0.04272 0.01672 0.819  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1348  0.02492 0.0278  0.8125 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1823  0.02545 0.02505 0.767  ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1758  0.0238  0.02455 0.776  ]] tensor([[19.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09106 0.05353 0.03098 0.824  ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08594 0.046   0.03995 0.828  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.06915 0.05823 0.04327 0.8296 ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1091  0.02396 0.0225  0.8447 ]] tensor([[20.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0819 0.0529 0.0246 0.8403]] tensor([[20.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1045  0.0605  0.02562 0.8096 ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0956   0.03802  0.014656 0.8516  ]] tensor([[20.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08514 0.0428  0.01246 0.86   ]] tensor([[20.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0715  0.06506 0.01984 0.8438 ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0923  0.0634  0.02191 0.8223 ]] tensor([[20.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1188  0.0791  0.01558 0.7866 ]] tensor([[20.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0795  0.04977 0.01566 0.855  ]] tensor([[20.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1137  0.02383 0.00877 0.8535 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.174    0.02545  0.008934 0.7915  ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.097   0.02911 0.00945 0.8643 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1757  0.0166  0.00809 0.8    ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1483  0.01772 0.00683 0.827  ]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1293   0.02176  0.006332 0.843   ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1373   0.01517  0.006325 0.8413  ]] tensor([[19.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1444   0.01837  0.005962 0.831   ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1348   0.02036  0.006207 0.839   ]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1776   0.01962  0.006573 0.7964  ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1357   0.02435  0.008026 0.832   ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1166  0.02684 0.00871 0.8477 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1936  0.0321  0.00864 0.7656 ]] tensor([[19.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1959  0.031   0.01022 0.7627 ]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1859   0.024    0.007324 0.7827  ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.19    0.02693 0.00809 0.775  ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2292   0.02342  0.007256 0.74    ]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2065   0.04004  0.009514 0.7437  ]] tensor([[19.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2205  0.03656 0.00855 0.7344 ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3328  0.02908 0.00649 0.632  ]] tensor([[20.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3105  0.02672 0.0051  0.6577 ]] tensor([[20.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2494  0.0243  0.00464 0.7217 ]] tensor([[20.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.217    0.02148  0.003975 0.7573  ]] tensor([[20.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2142   0.02223  0.004177 0.7593  ]] tensor([[20.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.262   0.02219 0.00404 0.712  ]] tensor([[20.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.211    0.0252   0.003689 0.7603  ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2228  0.02008 0.00333 0.754  ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.308    0.01582  0.003117 0.673   ]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3054   0.01385  0.003187 0.6777  ]] tensor([[19.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2673   0.01289  0.004745 0.7153  ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1583   0.01952  0.005093 0.817   ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2089  0.02101 0.00621 0.764  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1401   0.02254  0.004948 0.8325  ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1786  0.02573 0.00749 0.788  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.245    0.0275   0.007286 0.72    ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2446  0.02788 0.00837 0.719  ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2042  0.01675 0.00856 0.7705 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1958   0.01087  0.006805 0.7866  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3289   0.02238  0.004917 0.644   ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3318   0.010666 0.00805  0.6494  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1655   0.014465 0.00492  0.815   ]] tensor([[20.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2233   0.02667  0.006435 0.7437  ]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2798  0.03613 0.01269 0.6714 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2452  0.03424 0.01094 0.7095 ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1736  0.02661 0.00979 0.79   ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1996   0.01639  0.007046 0.777   ]] tensor([[20.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2913  0.0113  0.00936 0.688  ]] tensor([[20.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1652  0.00992 0.01178 0.813  ]] tensor([[20.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1584  0.02283 0.01385 0.8047 ]] tensor([[20.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.138   0.017   0.01283 0.832  ]] tensor([[20.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.264   0.01377 0.01585 0.7065 ]] tensor([[20.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2957  0.01202 0.01543 0.677  ]] tensor([[20.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3188  0.01717 0.00978 0.6543 ]] tensor([[20.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4514  0.02711 0.00982 0.5117 ]] tensor([[19.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3213  0.01001 0.00912 0.6597 ]] tensor([[20.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3323  0.0168  0.01035 0.6406 ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2761   0.013535 0.01607  0.6943  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2374  0.01949 0.01164 0.7314 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1627  0.0238  0.01255 0.801  ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1145  0.02118 0.01811 0.846  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1913   0.0251   0.014984 0.7686  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.359    0.02519  0.015045 0.601   ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4492  0.04446 0.01295 0.4934 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3433  0.041   0.01331 0.6025 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3577  0.03598 0.01672 0.59   ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3723  0.02455 0.01768 0.5854 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4255  0.03082 0.0222  0.5215 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3975  0.02621 0.01566 0.5605 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4368  0.02881 0.01567 0.5186 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4917  0.0379  0.01581 0.4546 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4248  0.03772 0.01674 0.5205 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3875  0.03384 0.01526 0.5635 ]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.386  0.0271 0.0167 0.5703]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.362   0.02705 0.01425 0.5967 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2294  0.02657 0.0149  0.729  ]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.19    0.031   0.01608 0.763  ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1992  0.03357 0.01513 0.752  ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1907  0.03815 0.01692 0.7544 ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1516   0.03128  0.010475 0.8066  ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1752  0.01875 0.00845 0.7974 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0779  0.01464 0.01582 0.8916 ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1074  0.01572 0.01839 0.8584 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08044 0.02531 0.01584 0.8784 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2106  0.0222  0.02052 0.7466 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1676   0.02002  0.012924 0.7993  ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1759  0.02133 0.01444 0.788  ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.135    0.013794 0.01125  0.84    ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0988   0.007504 0.012764 0.881   ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0947   0.007534 0.01302  0.885   ]] tensor([[18.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0917  0.0113  0.01342 0.884  ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0975   0.00893  0.011284 0.8823  ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.12317 0.01258 0.0095  0.855  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1571   0.009285 0.01069  0.8228  ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1874  0.01091 0.01256 0.789  ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1603  0.01337 0.01217 0.814  ]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1576  0.01662 0.01254 0.813  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0693  0.01046 0.00777 0.9126 ]] tensor([[18.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05392  0.010956 0.00867  0.9263  ]] tensor([[18.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05167  0.010826 0.007557 0.93    ]] tensor([[18.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03455  0.006695 0.01054  0.948   ]] tensor([[18.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09033 0.01242 0.01282 0.8843 ]] tensor([[17.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04498  0.011024 0.01192  0.932   ]] tensor([[17.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03537  0.00938  0.013855 0.9414  ]] tensor([[17.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0406   0.009056 0.01182  0.9385  ]] tensor([[18.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0751   0.01098  0.013885 0.9     ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.06854  0.014145 0.014595 0.903   ]] tensor([[18.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0618   0.01491  0.015144 0.908   ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02281  0.010445 0.01183  0.955   ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04202 0.01475 0.01645 0.927  ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04428 0.01086 0.0125  0.932  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1368  0.0142  0.01089 0.838  ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0338  0.01388 0.00939 0.943  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.06213  0.012825 0.01205  0.913   ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03983 0.01443 0.01023 0.9355 ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04742 0.01857 0.01058 0.9233 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0771  0.01566 0.01145 0.896  ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07196 0.01311 0.01021 0.905  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0637  0.01587 0.01275 0.9077 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1177   0.02751  0.012024 0.843   ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0806   0.012955 0.01236  0.894   ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0974  0.02243 0.01182 0.868  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0794  0.0292  0.01074 0.881  ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1577  0.03925 0.01466 0.7886 ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1123   0.03424  0.010445 0.843   ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10046  0.0321   0.013176 0.8545  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1359   0.0333   0.011505 0.8193  ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4888  0.02322 0.00676 0.4812 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1316   0.04077  0.008545 0.819   ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1033   0.02234  0.009605 0.8647  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1049   0.03406  0.009605 0.8516  ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1268  0.04813 0.01125 0.814  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2593  0.03506 0.01212 0.694  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1862  0.02945 0.01307 0.7715 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1547  0.03616 0.01121 0.798  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1208  0.03152 0.00903 0.839  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.11554  0.02498  0.005844 0.8535  ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08704 0.02495 0.00876 0.8794 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1595   0.02126  0.009285 0.81    ]] tensor([[18.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1126   0.02289  0.005875 0.8584  ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0855   0.03296  0.004326 0.8774  ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09924  0.02432  0.005775 0.8706  ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1876   0.02661  0.007866 0.778   ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3867   0.04272  0.008026 0.5625  ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3203  0.02357 0.00895 0.647  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4512   0.02751  0.010284 0.511   ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.629   0.02438 0.01016 0.3367 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.672   0.02228 0.00782 0.298  ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.559   0.02417 0.00797 0.409  ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4585   0.02248  0.007645 0.511   ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5806 0.0251 0.0079 0.3867]] tensor([[19.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6104   0.02155  0.009125 0.359   ]] tensor([[20.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6543  0.02786 0.00904 0.309  ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.703   0.04935 0.0121  0.2355 ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6064  0.03162 0.01128 0.3508 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6294  0.0344  0.01001 0.3264 ]] tensor([[20.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.654   0.03009 0.00761 0.3088 ]] tensor([[19.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.789   0.01976 0.00662 0.1846 ]] tensor([[20.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6245 0.0326 0.0085 0.3342]] tensor([[20.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7305  0.04056 0.00994 0.2192 ]] tensor([[20.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6724   0.2051   0.003756 0.1187  ]] tensor([[20.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.63    0.06046 0.00711 0.3022 ]] tensor([[20.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4248 0.0515 0.0115 0.512 ]] tensor([[20.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3738  0.03214 0.01543 0.5786 ]] tensor([[20.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3271  0.0319  0.01051 0.6304 ]] tensor([[20.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4324   0.04556  0.008835 0.513   ]] tensor([[20.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2542   0.03082  0.013466 0.7017  ]] tensor([[20.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2776  0.03644 0.00921 0.677  ]] tensor([[20.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.297   0.03384 0.01016 0.659  ]] tensor([[20.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4875  0.02246 0.00981 0.48   ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.569   0.03418 0.01181 0.385  ]] tensor([[20.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.619    0.02255  0.010826 0.3474  ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5625 0.0225 0.0162 0.399 ]] tensor([[20.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5386  0.02682 0.01482 0.4197 ]] tensor([[20.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6255  0.02042 0.01404 0.34   ]] tensor([[20.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4656  0.04263 0.01892 0.473  ]] tensor([[20.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3315  0.0449  0.02292 0.6006 ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2391 0.0542 0.0256 0.681 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.325   0.04465 0.02353 0.607  ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3074  0.04294 0.01875 0.631  ]] tensor([[20.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2688  0.03314 0.03262 0.6655 ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1779  0.035   0.03845 0.7485 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.229   0.04105 0.02452 0.7056 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1796  0.03375 0.03072 0.756  ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2632  0.03732 0.0379  0.6616 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2411  0.0406  0.03162 0.6865 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2122  0.03574 0.02307 0.729  ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2986  0.03796 0.02162 0.642  ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.356   0.0412  0.01588 0.587  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4355  0.03412 0.01336 0.517  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4045   0.03023  0.012405 0.5527  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3245  0.04257 0.0172  0.6157 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.411   0.03592 0.01723 0.536  ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.369   0.0298  0.02113 0.58   ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2822  0.0376  0.02428 0.656  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3308  0.04617 0.02434 0.5986 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2532  0.0382  0.02046 0.688  ]] tensor([[20.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3188  0.04965 0.01689 0.6147 ]] tensor([[20.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3936  0.05496 0.02187 0.53   ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5166  0.05444 0.0142  0.415  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.525    0.0756   0.015366 0.384   ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6133  0.05792 0.01584 0.3132 ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5312  0.0697  0.01656 0.3826 ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4436  0.066   0.01804 0.4724 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.551   0.04965 0.01538 0.3843 ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4856  0.04883 0.01662 0.449  ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.43   0.0747 0.0159 0.4795]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4314  0.08234 0.01956 0.4666 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3901  0.07446 0.01854 0.517  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5664  0.0527  0.01486 0.3657 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5693  0.0432  0.01403 0.3733 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.532   0.0451  0.01419 0.4082 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.519   0.05472 0.01567 0.4106 ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3806  0.0641  0.01865 0.5366 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3735  0.0649  0.01831 0.5435 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2646  0.06793 0.02277 0.6445 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3062  0.07983 0.02397 0.59   ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.378   0.06168 0.0191  0.5415 ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3523  0.0662  0.01839 0.563  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4082  0.0527  0.01487 0.5244 ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4238  0.04755 0.00922 0.5195 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.325    0.0468   0.011475 0.6167  ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3345  0.0482  0.02074 0.5967 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2734  0.04752 0.02316 0.656  ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2915  0.04987 0.02213 0.6367 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.355   0.0374  0.02234 0.5854 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.446   0.04486 0.019   0.49   ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.356   0.05127 0.01516 0.5776 ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4329  0.0252  0.01136 0.5303 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4746  0.04773 0.01032 0.4673 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3406  0.0468  0.01496 0.5977 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.302   0.04282 0.016   0.639  ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3477  0.0442  0.02599 0.582  ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3088  0.03687 0.03058 0.6235 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3718  0.03738 0.02377 0.567  ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3762  0.03394 0.02483 0.565  ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3162  0.0318  0.02292 0.629  ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3982  0.03168 0.02586 0.5444 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4463  0.03723 0.01842 0.498  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3682  0.03476 0.02708 0.5703 ]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3923  0.03375 0.02931 0.5444 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1897  0.03677 0.0351  0.7383 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5312  0.0494  0.02484 0.3948 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5996   0.04346  0.015015 0.3418  ]] tensor([[19.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7285  0.04047 0.01215 0.2188 ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3943  0.04935 0.03394 0.5225 ]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.63    0.03287 0.02559 0.3118 ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.387   0.03128 0.02759 0.554  ]] tensor([[20.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6265  0.03647 0.01723 0.3198 ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4668  0.02847 0.01548 0.4893 ]] tensor([[19.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4683  0.0226  0.01845 0.4907 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.744   0.05304 0.01496 0.1881 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5723   0.02939  0.011505 0.387   ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6416  0.03677 0.00901 0.3127 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4236  0.06494 0.0239  0.4875 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3552  0.0678  0.01825 0.5586 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3523  0.06726 0.02592 0.554  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4307  0.06107 0.02046 0.4878 ]] tensor([[20.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6016  0.04224 0.01874 0.3374 ]] tensor([[19.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4602  0.0541  0.03958 0.446  ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.474   0.0389  0.04843 0.4385 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6733  0.02736 0.04373 0.2556 ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4731  0.03537 0.05392 0.4375 ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.286   0.05457 0.0819  0.5776 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2241 0.0333 0.0524 0.6904]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2646  0.02702 0.04318 0.665  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2339  0.03644 0.03165 0.698  ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.275   0.02943 0.0463  0.6494 ]] tensor([[18.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3066  0.02478 0.05853 0.61   ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1334  0.0332  0.04132 0.792  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1069 0.0375 0.0537 0.802 ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0692  0.04538 0.0556  0.83   ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.098  0.0415 0.0396 0.821 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1615  0.03836 0.04147 0.759  ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04773 0.0366  0.0428  0.873  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04855 0.04285 0.04782 0.861  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.06555 0.0731  0.05026 0.811  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1302  0.07776 0.0543  0.738  ]] tensor([[20.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.295   0.0746  0.06183 0.5684 ]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5405  0.10815 0.0527  0.2986 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5044  0.04764 0.04916 0.399  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.412   0.04208 0.05664 0.4893 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4766  0.05966 0.0625  0.4014 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5195 0.0748 0.0433 0.3625]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3574 0.0602 0.054  0.5283]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5317  0.04868 0.04868 0.371  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3345  0.0712  0.05997 0.534  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3845  0.04813 0.0734  0.494  ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6196  0.0435  0.05762 0.2793 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6006 0.074  0.055  0.2705]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4966  0.0762  0.05234 0.3748 ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4797  0.0564  0.06003 0.4038 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5625  0.05154 0.07495 0.3108 ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.463   0.0444  0.05792 0.4348 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.633   0.04373 0.06366 0.2598 ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.579   0.0888  0.09454 0.2377 ]] tensor([[18.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.748  0.0361 0.0614 0.1544]] tensor([[18.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6084  0.0484  0.07733 0.2659 ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5166 0.0536 0.1117 0.318 ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4922  0.05435 0.1098  0.3435 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5083  0.05112 0.1226  0.318  ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6206  0.05774 0.07074 0.2507 ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6094  0.1059  0.07056 0.214  ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7085  0.06094 0.05133 0.1792 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5273  0.05222 0.05222 0.3682 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6367 0.0835 0.0491 0.2306]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.614   0.05533 0.0411  0.2898 ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.594   0.0607  0.05112 0.2942 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5967  0.05725 0.06    0.2861 ]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4905  0.0685  0.04706 0.394  ]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3132 0.0898 0.0884 0.509 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.495  0.0796 0.0629 0.3623]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5083  0.0744  0.07324 0.344  ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.657   0.0476  0.04987 0.2455 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.681   0.05093 0.05338 0.2144 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.571   0.044   0.06015 0.3252 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5923 0.0686 0.0719 0.267 ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.569   0.0735  0.05292 0.3047 ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7163  0.0458  0.04507 0.1927 ]] tensor([[18.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7354  0.03604 0.0485  0.1802 ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.821   0.03232 0.03778 0.1093 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8794  0.01662 0.02069 0.0831 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.92     0.01822  0.007595 0.05438 ]] tensor([[18.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7363  0.04355 0.0422  0.1777 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.677   0.04605 0.04327 0.2339 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6064  0.04752 0.0639  0.282  ]] tensor([[18.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5073  0.05515 0.04868 0.389  ]] tensor([[18.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.583   0.05957 0.05957 0.2979 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4294  0.077   0.05722 0.4363 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.548   0.07654 0.0534  0.322  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.683   0.0561  0.04233 0.2184 ]] tensor([[18.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.741   0.04382 0.03308 0.1816 ]] tensor([[17.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.626   0.049   0.04752 0.2776 ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.534   0.04892 0.05542 0.3616 ]] tensor([[18.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4353 0.0647 0.0844 0.4155]] tensor([[18.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5234  0.06354 0.05875 0.3542 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4358  0.07336 0.09424 0.3967 ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4172  0.0931  0.05917 0.4304 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4614 0.073  0.0319 0.4336]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.441   0.06866 0.04947 0.441  ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4546  0.05515 0.0697  0.4204 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5557  0.0752  0.05765 0.3115 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6245  0.05902 0.06384 0.2524 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5996  0.1075  0.04697 0.2461 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.75    0.05695 0.03348 0.1597 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.703   0.06238 0.02684 0.2078 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.631   0.06445 0.02448 0.28   ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.739   0.05353 0.01793 0.1897 ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8164  0.02666 0.01044 0.1464 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.706   0.04175 0.01584 0.2365 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7495 0.0479 0.0163 0.1865]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7075  0.05124 0.02238 0.2191 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7876  0.0792  0.02306 0.11   ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8726  0.0408  0.01524 0.0716 ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.779   0.06696 0.02927 0.1251 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7856  0.0708  0.01735 0.1262 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.69    0.0919  0.02634 0.1917 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7285  0.08044 0.02342 0.1677 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8267  0.04736 0.02238 0.10345]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8467  0.0355  0.01813 0.09955]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7847 0.0457 0.0373 0.1322]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6836  0.05972 0.03146 0.2253 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.814    0.09717  0.005836 0.0831  ]] tensor([[20.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.876    0.04163  0.008194 0.0742  ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.861   0.02258 0.02023 0.09656]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.875   0.01933 0.01628 0.0894 ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7886  0.11725 0.01357 0.08057]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.805   0.11066 0.00952 0.0749 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8257   0.105    0.008354 0.06076 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8057  0.1255  0.01047 0.05835]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8574  0.0918  0.00815 0.0427 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.914   0.0492  0.00351 0.0333 ]] tensor([[19.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.928   0.03543 0.0043  0.03226]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.935   0.0278  0.00293 0.03406]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9478  0.01906 0.00235 0.03094]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8486  0.02728 0.01437 0.1096 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9033  0.01051 0.0121  0.07416]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8823  0.01279 0.01164 0.093  ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8677   0.01746  0.009346 0.1053  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8335  0.02402 0.0083  0.134  ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7686  0.02934 0.01063 0.1913 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.826   0.02917 0.00977 0.1349 ]] tensor([[19.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.663   0.03983 0.02034 0.2766 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6167  0.0307  0.03268 0.3198 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.557   0.02647 0.03345 0.3828 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.529   0.01672 0.05573 0.399  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8604  0.01625 0.01247 0.1111 ]] tensor([[20.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7554  0.01921 0.04327 0.1823 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8438  0.01595 0.01671 0.1235 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.803   0.01182 0.02426 0.1606 ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8213  0.0105  0.02121 0.1472 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.912   0.00814 0.00605 0.0737 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.862    0.01816  0.007114 0.11304 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.923    0.01915  0.005074 0.0529  ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8345   0.0304   0.007217 0.1279  ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8687   0.02464  0.007633 0.099   ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8647  0.01442 0.0142  0.10657]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.755  0.0239 0.0302 0.1909]] tensor([[18.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.728   0.02869 0.0278  0.2152 ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.643   0.05038 0.02533 0.281  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.784   0.02481 0.01903 0.1722 ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7026  0.01872 0.04352 0.2354 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6245  0.02312 0.03812 0.3142 ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.558  0.0315 0.0325 0.3777]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.393   0.02083 0.02324 0.563  ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.317   0.01845 0.03445 0.6304 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.514   0.02156 0.04425 0.42   ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.582  0.0272 0.0435 0.3474]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.609   0.02437 0.04553 0.321  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6504  0.01602 0.0361  0.2976 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6216 0.0203 0.0356 0.3225]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.614   0.01797 0.0689  0.2993 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6753  0.02786 0.0563  0.2407 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.605   0.02536 0.06082 0.3088 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5894  0.02322 0.04074 0.3464 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5083  0.03004 0.04044 0.4214 ]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4683  0.04288 0.035   0.4539 ]] tensor([[20.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5405  0.03735 0.04504 0.3772 ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5186  0.02747 0.05054 0.4036 ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.739   0.01139 0.01738 0.2324 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7607  0.00638 0.02158 0.2113 ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.795    0.007095 0.01226  0.1859  ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8086   0.006886 0.01787  0.1669  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.711   0.01453 0.02081 0.2537 ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7427  0.01447 0.03366 0.2095 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.693   0.02486 0.03503 0.2471 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7305  0.02661 0.02702 0.216  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.677   0.03217 0.02583 0.2651 ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.785   0.02193 0.0123  0.1808 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7686 0.0317 0.0143 0.1854]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.773   0.03192 0.01707 0.178  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.82    0.0308  0.01116 0.1381 ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.907   0.01178 0.00661 0.07446]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8574  0.01014 0.01063 0.12164]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8604  0.01625 0.01758 0.106  ]] tensor([[18.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7905  0.02582 0.01258 0.171  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7046  0.0216  0.01877 0.2551 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8022  0.01639 0.01317 0.1682 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.895   0.01359 0.01564 0.0758 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8516  0.01796 0.022   0.1083 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7456   0.02287  0.014534 0.217   ]] tensor([[20.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7666  0.0106  0.01979 0.2031 ]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.794   0.02051 0.02893 0.1564 ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.915    0.00816  0.007435 0.06946 ]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8545  0.0154  0.0183  0.11206]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.802   0.02136 0.02875 0.1483 ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5884  0.06396 0.06104 0.2866 ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.628   0.0774  0.05237 0.2422 ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7134  0.0517  0.04562 0.1891 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7974  0.04294 0.02524 0.1343 ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8247  0.04172 0.02693 0.1065 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.724   0.04556 0.0384  0.1919 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.736   0.039   0.02403 0.2012 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5874  0.046   0.04745 0.3193 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6406  0.03967 0.0486  0.2712 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.687   0.02208 0.02208 0.269  ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.652   0.03854 0.03348 0.2761 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7905  0.037   0.02141 0.1509 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.725   0.04565 0.02768 0.2014 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.764   0.03152 0.01421 0.1902 ]] tensor([[20.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.781   0.01924 0.02017 0.1798 ]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7656  0.0147  0.01666 0.2029 ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.888   0.01118 0.00857 0.09216]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7124  0.01265 0.02477 0.25   ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.662   0.01291 0.01685 0.3079 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7676  0.01617 0.02545 0.191  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5605  0.01694 0.037   0.3853 ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5854  0.0156  0.03802 0.3608 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7686  0.0157  0.01808 0.1974 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6704  0.01528 0.0304  0.284  ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.661   0.01875 0.02226 0.2979 ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5337  0.02345 0.02069 0.422  ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5664  0.02374 0.02095 0.3892 ]] tensor([[18.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7466  0.01869 0.01032 0.2242 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.588   0.02882 0.02109 0.3623 ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.62    0.04092 0.01706 0.3218 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.803    0.01775  0.012985 0.1658  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.474   0.05154 0.03598 0.4385 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.555   0.0463  0.02292 0.3757 ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3936  0.04083 0.019   0.5464 ]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3813  0.03604 0.01898 0.5635 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3098  0.053   0.02109 0.616  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2578  0.02289 0.01839 0.7007 ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3694  0.01624 0.01478 0.5996 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4597  0.00984 0.00969 0.521  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4219  0.03056 0.01399 0.533  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6304   0.02948  0.013084 0.3271  ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6426  0.01535 0.00848 0.3335 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4272  0.03094 0.01005 0.5317 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3472  0.06726 0.01346 0.5723 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3975  0.04974 0.00964 0.543  ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2847  0.04166 0.01157 0.662  ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4062  0.02055 0.00898 0.564  ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4324  0.02556 0.01227 0.53   ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2769  0.03806 0.0109  0.6743 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2903   0.04184  0.013374 0.6543  ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3618  0.05548 0.01381 0.569  ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.371   0.0665  0.01395 0.5483 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4485   0.0636   0.010544 0.4773  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5034  0.064   0.00853 0.424  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4473  0.0675  0.00914 0.476  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4395  0.0456  0.00898 0.506  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3306   0.0593   0.011314 0.5986  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3198  0.05222 0.01112 0.6167 ]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2825  0.0447  0.01595 0.6567 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3318  0.05676 0.01083 0.6006 ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2964  0.0557  0.01063 0.637  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3723  0.04962 0.01024 0.568  ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4692  0.05023 0.01138 0.4692 ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4434  0.04974 0.01219 0.4946 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4324  0.05005 0.01226 0.5054 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3396  0.0572  0.01639 0.587  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3037  0.05707 0.01585 0.6235 ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2856  0.06476 0.01563 0.634  ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3196  0.06696 0.01666 0.5967 ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4297  0.04745 0.01277 0.5103 ]] tensor([[19.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.497   0.05322 0.01081 0.4387 ]] tensor([[19.2344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.493   0.0627  0.00917 0.435  ]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4639  0.03928 0.01091 0.486  ]] tensor([[19.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3276  0.0216  0.00929 0.6416 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.547    0.01307  0.007107 0.4329  ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5083   0.013756 0.00809  0.47    ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.299   0.01883 0.00785 0.6743 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3215  0.0193  0.00971 0.6494 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3289   0.01974  0.007732 0.6436  ]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.396   0.03406 0.01159 0.5586 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4446   0.04404  0.007416 0.504   ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.533    0.02696  0.004684 0.435   ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.456    0.02342  0.003765 0.5166  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.356    0.0224   0.006725 0.6147  ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3494  0.02913 0.00835 0.6133 ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3865  0.02629 0.0073  0.58   ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3574   0.02933  0.005424 0.608   ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.261    0.02504  0.004704 0.7095  ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2094   0.0204   0.004696 0.7656  ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.206   0.02312 0.00508 0.7656 ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2384   0.02289  0.004166 0.7344  ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.215   0.01793 0.00475 0.762  ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2883  0.01459 0.00512 0.692  ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1842   0.01039  0.005146 0.8003  ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1434  0.0142  0.00433 0.838  ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1434   0.01334  0.004612 0.8384  ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.11694  0.01463  0.004395 0.8643  ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0655   0.012115 0.004253 0.918   ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0757   0.01254  0.004337 0.907   ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0511   0.01122  0.003586 0.934   ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0814   0.01249  0.002966 0.9033  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1205  0.01417 0.00262 0.863  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10315  0.01607  0.003586 0.8774  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0991   0.01428  0.002901 0.884   ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1078   0.01392  0.003412 0.875   ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1237   0.01388  0.003794 0.8584  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.085    0.012054 0.003244 0.9     ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0727   0.01097  0.003046 0.913   ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0899  0.01236 0.00395 0.894  ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05258 0.01249 0.00306 0.9316 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05652  0.013214 0.003906 0.9263  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.081   0.01596 0.00436 0.8984 ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0721   0.01712  0.004753 0.9062  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0753  0.01707 0.00412 0.9033 ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.06635  0.01306  0.004807 0.916   ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07043  0.011505 0.0043   0.9136  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04086  0.010666 0.00357  0.945   ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0541   0.01246  0.003918 0.9297  ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05835 0.01062 0.00416 0.927  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07043  0.012634 0.003244 0.9136  ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0721  0.01767 0.00426 0.906  ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05988  0.01357  0.004337 0.9224  ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07635  0.01842  0.003628 0.9014  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.072    0.0185   0.004192 0.9053  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0649  0.02074 0.00435 0.91   ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0469   0.02148  0.003792 0.9277  ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0442   0.02153  0.004375 0.9297  ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02904  0.01965  0.004593 0.947   ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03427  0.02177  0.003445 0.9404  ]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02702 0.01718 0.00333 0.9526 ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02187 0.01575 0.00296 0.9595 ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02913 0.0174  0.00382 0.9497 ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03326  0.0205   0.004097 0.9424  ]] tensor([[18.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03482  0.02016  0.003847 0.9414  ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0376   0.0189   0.003286 0.9404  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02623 0.01642 0.00355 0.9536 ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03992  0.01915  0.003544 0.9375  ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02773  0.02159  0.003525 0.9473  ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.01848 0.01605 0.00316 0.9624 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02359  0.01698  0.002337 0.957   ]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03099  0.01738  0.002665 0.9487  ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03995 0.01859 0.00318 0.9385 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05368  0.02037  0.003653 0.9224  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04572 0.01762 0.00406 0.9326 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05124 0.0217  0.00441 0.923  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04486  0.0212   0.004173 0.9297  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05148  0.01807  0.003672 0.927   ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0315   0.015594 0.00307  0.9497  ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03702  0.01982  0.003038 0.94    ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03079  0.02324  0.003096 0.943   ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02861 0.02028 0.00381 0.9473 ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.02222  0.015274 0.002739 0.96    ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.04178  0.01883  0.002981 0.9365  ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.03537 0.02016 0.00329 0.9414 ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10974 0.01053 0.00297 0.877  ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07166 0.00897 0.00424 0.915  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0541   0.012856 0.0032   0.9297  ]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1094   0.01347  0.003407 0.8735  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10767  0.0148   0.003983 0.8735  ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1111   0.01247  0.002455 0.874   ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1285   0.03052  0.003119 0.838   ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.069    0.01945  0.002937 0.9087  ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09625  0.02982  0.002817 0.871   ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0828   0.02336  0.003582 0.89    ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.05673  0.0244   0.003302 0.9155  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0831   0.03308  0.003769 0.88    ]] tensor([[18.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0832   0.0326   0.003654 0.8804  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0996   0.03552  0.004242 0.861   ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09705  0.03406  0.003883 0.865   ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09937  0.03772  0.004646 0.8584  ]] tensor([[18.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09314  0.04538  0.005173 0.8564  ]] tensor([[18.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07666  0.04105  0.004677 0.8774  ]] tensor([[18.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0678  0.0481  0.00476 0.8794 ]] tensor([[18.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09045  0.0455   0.005783 0.8584  ]] tensor([[18.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0628  0.0381  0.00448 0.8945 ]] tensor([[18.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0707   0.04935  0.005535 0.8745  ]] tensor([[17.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0863   0.0484   0.007195 0.858   ]] tensor([[17.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08417 0.04507 0.00702 0.864  ]] tensor([[17.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0933   0.0414   0.006977 0.8584  ]] tensor([[18.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09106 0.03857 0.0062  0.8643 ]] tensor([[18.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08105  0.0414   0.005966 0.8716  ]] tensor([[18.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08276  0.04865  0.006187 0.8623  ]] tensor([[18.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07806  0.05038  0.005833 0.8657  ]] tensor([[18.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.09235  0.0518   0.006794 0.849   ]] tensor([[17.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0886  0.0513  0.00594 0.854  ]] tensor([[17.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0983   0.04645  0.005905 0.849   ]] tensor([[18.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07983  0.0434   0.005104 0.8716  ]] tensor([[18.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0949  0.05325 0.00579 0.846  ]] tensor([[18.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10004 0.04303 0.0062  0.8506 ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1249   0.0418   0.006313 0.827   ]] tensor([[18.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1313  0.0447  0.00644 0.8174 ]] tensor([[17.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1069  0.04596 0.00642 0.841  ]] tensor([[18.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1274   0.03485  0.006546 0.831   ]] tensor([[17.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1475   0.03613  0.006382 0.81    ]] tensor([[17.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1327  0.03357 0.00773 0.826  ]] tensor([[17.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10443  0.04153  0.006676 0.847   ]] tensor([[17.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.11444  0.04697  0.006065 0.8325  ]] tensor([[17.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1042  0.05753 0.00606 0.832  ]] tensor([[17.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.10986  0.0721   0.006104 0.812   ]] tensor([[17.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1835  0.0982  0.00418 0.7144 ]] tensor([[18.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1525   0.09247  0.004604 0.7505  ]] tensor([[18.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1973  0.0876  0.00457 0.7104 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.134    0.04086  0.004307 0.821   ]] tensor([[18.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0899  0.0399  0.00389 0.866  ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0547  0.04395 0.00478 0.8965 ]] tensor([[18.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1035  0.05124 0.0054  0.84   ]] tensor([[17.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1204   0.0379   0.005547 0.836   ]] tensor([[18.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0752  0.03183 0.00528 0.8877 ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0953  0.02298 0.00581 0.876  ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0652  0.02634 0.00855 0.9    ]] tensor([[18.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.07166  0.0215   0.006557 0.9004  ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1114   0.04642  0.006187 0.836   ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0868  0.02908 0.00724 0.877  ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0771  0.02042 0.00663 0.896  ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1071  0.01804 0.00559 0.869  ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1254  0.01241 0.00577 0.8564 ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0912  0.01141 0.00491 0.8926 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.0601   0.010124 0.00422  0.926   ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.08466  0.01494  0.004486 0.896   ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.1411  0.01608 0.00468 0.838  ]] tensor([[18.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2617   0.02217  0.004574 0.7114  ]] tensor([[18.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2406  0.02576 0.00414 0.7295 ]] tensor([[18.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2258  0.02872 0.00476 0.7407 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2903  0.02069 0.00371 0.6855 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2285   0.01735  0.004894 0.749   ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2456   0.01698  0.004227 0.7334  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2874   0.02017  0.003294 0.6895  ]] tensor([[18.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.2817   0.01802  0.002678 0.6973  ]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3252   0.01474  0.003239 0.6567  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.428    0.01851  0.003881 0.55    ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4045   0.01241  0.003393 0.5796  ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4404  0.02446 0.00381 0.5312 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.3303   0.01807  0.005016 0.6465  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4644   0.02106  0.004414 0.5103  ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4954   0.02838  0.003782 0.4727  ]] tensor([[18.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.624    0.01797  0.002672 0.3555  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.525   0.02306 0.00312 0.449  ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.48     0.02075  0.004284 0.495   ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.4365   0.02243  0.006226 0.5347  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.465   0.01721 0.00695 0.5107 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5703   0.01341  0.005505 0.411   ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5513   0.01359  0.005493 0.4294  ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5464   0.01811  0.003567 0.4321  ]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.59    0.01283 0.00423 0.393  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5176   0.01275  0.006023 0.4639  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5293  0.01    0.00767 0.453  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.5747   0.012695 0.004894 0.4077  ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.662   0.01332 0.00691 0.3176 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.656   0.01775 0.00717 0.3196 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.588   0.02245 0.00996 0.3796 ]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.744   0.01834 0.00696 0.2306 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6025   0.02864  0.008736 0.3599  ]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.602   0.02228 0.01036 0.3652 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6875   0.013824 0.00788  0.291   ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8125   0.008095 0.0063   0.173   ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.753   0.00933 0.00462 0.2333 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7524   0.00962  0.004547 0.2332  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7617   0.011566 0.00475  0.2217  ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8223   0.01229  0.003576 0.1619  ]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.801   0.01636 0.00447 0.1786 ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.852    0.01356  0.003649 0.1306  ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.856    0.013405 0.003666 0.1272  ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8643   0.0114   0.003702 0.12067 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.877    0.01139  0.003696 0.10803 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.882    0.01026  0.002806 0.1053  ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8984   0.00983  0.002523 0.089   ]] tensor([[19.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.884    0.010956 0.002995 0.1023  ]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.885    0.00999  0.002565 0.1024  ]] tensor([[19.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.908    0.00933  0.001986 0.0806  ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.887    0.01065  0.002823 0.09955 ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.871    0.00968  0.002953 0.1161  ]] tensor([[19.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.841    0.011444 0.004017 0.1438  ]] tensor([[19.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.824   0.01374 0.00498 0.1572 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7896   0.01218  0.005074 0.1935  ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8154  0.01128 0.005   0.1683 ]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7837  0.00971 0.00504 0.2013 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7354   0.01152  0.006565 0.2463  ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7305  0.01588 0.00532 0.2485 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7114   0.01987  0.007195 0.2617  ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7217   0.01836  0.006344 0.2534  ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.578    0.0183   0.006226 0.3975  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.631    0.015305 0.00572  0.3484  ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.695   0.01535 0.00461 0.2852 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.728    0.01634  0.003704 0.2517  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.799    0.013535 0.0037   0.184   ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8135   0.01336  0.002672 0.1705  ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.784   0.01328 0.00431 0.1982 ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.759    0.01625  0.004044 0.2208  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.806    0.01622  0.003096 0.1743  ]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.807   0.0294  0.00444 0.1589 ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.853    0.01913  0.003223 0.1248  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.866    0.01491  0.003223 0.1154  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8945   0.01161  0.002674 0.0914  ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8843   0.0143   0.003614 0.0977  ]] tensor([[19.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9316   0.01229  0.002617 0.0534  ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.898    0.01241  0.003342 0.0862  ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8794   0.011604 0.00401  0.10504 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8047   0.01723  0.006542 0.1714  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8184   0.015465 0.0051   0.1611  ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.755   0.01668 0.00533 0.2231 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.687    0.01831  0.008125 0.2864  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6665  0.0192  0.00936 0.3052 ]] tensor([[18.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.698   0.0192  0.00921 0.2734 ]] tensor([[18.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.716    0.01353  0.003212 0.2676  ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.91     0.006325 0.001625 0.08203 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9126   0.00518  0.002335 0.0798  ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9126   0.00615  0.001484 0.0798  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.912    0.009514 0.001288 0.0772  ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.915    0.01049  0.001442 0.0728  ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.86    0.01246 0.00392 0.12384]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8667   0.010086 0.004337 0.1191  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8755   0.006477 0.00336  0.1148  ]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.881    0.004997 0.002361 0.11194 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.862    0.005543 0.002537 0.1301  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.879    0.005737 0.002111 0.11346 ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8667   0.006214 0.00236  0.1248  ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.788    0.007034 0.002508 0.2024  ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7793   0.009964 0.00403  0.2065  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7905   0.01219  0.003439 0.1937  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7573   0.01726  0.005104 0.2203  ]] tensor([[18.8438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.705    0.01907  0.004326 0.2717  ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.677    0.01747  0.005585 0.3003  ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.815    0.01058  0.003893 0.1708  ]] tensor([[18.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.831    0.013435 0.00416  0.1514  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7915  0.013   0.00729 0.188  ]] tensor([[18.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7896  0.01424 0.00864 0.1875 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8315  0.01672 0.00742 0.1445 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.836    0.02524  0.004456 0.1344  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8643   0.01534  0.003319 0.117   ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.867    0.01539  0.003832 0.1137  ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8745   0.01459  0.003105 0.1078  ]] tensor([[18.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8677   0.01888  0.003178 0.1103  ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8057  0.02432 0.00397 0.1663 ]] tensor([[18.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.697    0.02171  0.004276 0.277   ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.691    0.01756  0.003681 0.2878  ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.707    0.01743  0.003273 0.2725  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.775    0.01397  0.002544 0.2086  ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.791    0.02141  0.002722 0.1849  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.852   0.021   0.00218 0.1247 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.899    0.01698  0.001605 0.08234 ]] tensor([[19.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8877   0.01704  0.001824 0.09357 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9      0.01132  0.001251 0.0877  ]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9023   0.010025 0.001124 0.0866  ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.895    0.009056 0.001598 0.09436 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8647   0.01023  0.002178 0.1227  ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.86     0.01226  0.002165 0.1259  ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8364   0.013306 0.002832 0.1476  ]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.817   0.01341 0.0035  0.166  ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.759   0.01625 0.00444 0.2207 ]] tensor([[19.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8438   0.01752  0.002905 0.1356  ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8564   0.01697  0.002995 0.1234  ]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.882    0.01517  0.002636 0.10046 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9272   0.01096  0.001818 0.06018 ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.896   0.01182 0.00212 0.0901 ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.876    0.01351  0.002499 0.108   ]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.878    0.00945  0.002584 0.1099  ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8633  0.00974 0.00279 0.1243 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8833   0.010284 0.002405 0.1039  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9106   0.007755 0.002024 0.0796  ]] tensor([[18.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.9243   0.00695  0.001677 0.06696 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.916   0.00793 0.0021  0.07404]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.901    0.00792  0.001767 0.08923 ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.877   0.00795 0.00204 0.1132 ]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.796    0.01153  0.003801 0.189   ]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.732    0.014496 0.004704 0.249   ]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7725   0.01191  0.004738 0.211   ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8174  0.01045 0.00329 0.1687 ]] tensor([[18.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8125  0.01398 0.00327 0.1703 ]] tensor([[18.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.838    0.012726 0.00382  0.1456  ]] tensor([[18.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.877    0.01311  0.003473 0.1064  ]] tensor([[18.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8564   0.0167   0.003725 0.12335 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8594   0.02594  0.003918 0.11096 ]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8926  0.01912 0.00271 0.08563]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.907    0.01768  0.001984 0.0733  ]] tensor([[18.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8945   0.01639  0.002018 0.08716 ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.883   0.01591 0.00248 0.099  ]] tensor([[18.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.872    0.01927  0.002607 0.10583 ]] tensor([[18.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.875    0.02225  0.003012 0.09973 ]] tensor([[18.3906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.832    0.02513  0.004795 0.138   ]] tensor([[18.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.798   0.02336 0.006   0.1726 ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7666   0.02664  0.007057 0.2     ]] tensor([[18.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7666   0.02426  0.006134 0.2031  ]] tensor([[18.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.749   0.02995 0.00658 0.2146 ]] tensor([[18.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8384  0.02454 0.00454 0.1327 ]] tensor([[18.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8154   0.02386  0.005325 0.1556  ]] tensor([[18.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.84     0.0217   0.005318 0.1329  ]] tensor([[18.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8604  0.0148  0.00488 0.1201 ]] tensor([[18.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.854   0.018   0.00492 0.12305]] tensor([[18.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.8354   0.02728  0.007008 0.1301  ]] tensor([[18.6094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.772    0.0226   0.006893 0.1984  ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7207  0.02281 0.00729 0.2491 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.766    0.02011  0.007397 0.2062  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.747   0.02516 0.0071  0.2208 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6187  0.03436 0.01048 0.3364 ]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.579   0.02281 0.01221 0.3857 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.653   0.02066 0.01293 0.3132 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7      0.01808  0.011856 0.27    ]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7944   0.010155 0.00698  0.1886  ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.722    0.01262  0.007656 0.2576  ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6816  0.01437 0.01051 0.2932 ]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6934  0.01556 0.01121 0.28   ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.66    0.01706 0.01084 0.3118 ]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6265  0.01695 0.01095 0.346  ]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7275  0.10815 0.0656  0.0985 ]] tensor([[20.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.722   0.10565 0.06824 0.104  ]] tensor([[20.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.729  0.0942 0.0733 0.1034]] tensor([[20.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7173  0.10333 0.07104 0.1083 ]] tensor([[20.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.709   0.10376 0.0783  0.10876]] tensor([[20.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.71   0.0976 0.0835 0.1089]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.711   0.0993  0.08234 0.10736]] tensor([[20.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7075  0.1102  0.07697 0.10516]] tensor([[20.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.711   0.1025  0.08234 0.10406]] tensor([[20.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.707   0.09875 0.08575 0.10846]] tensor([[20.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.692   0.10443 0.09076 0.1129 ]] tensor([[20.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7134  0.0907  0.09503 0.10114]] tensor([[19.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7153  0.0953  0.08813 0.10144]] tensor([[19.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.738   0.1063  0.07306 0.08276]] tensor([[20.1719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7114 0.1276 0.0837 0.0774]] tensor([[20.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7207  0.1176  0.0847  0.07715]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7017 0.1382 0.0775 0.0825]] tensor([[20.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7017 0.1382 0.0775 0.0825]] tensor([[20.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7085  0.129   0.08075 0.082  ]] tensor([[20.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.717  0.1246 0.0768 0.0817]] tensor([[20.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.732   0.114   0.06915 0.0847 ]] tensor([[20.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.707   0.121   0.08057 0.0913 ]] tensor([[20.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7197 0.1104 0.0795 0.0901]] tensor([[20.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7134  0.1077  0.08386 0.09503]] tensor([[20.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6997  0.11786 0.0785  0.104  ]] tensor([[20.4531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6895  0.1256  0.08234 0.1025 ]] tensor([[20.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.714   0.1061  0.08795 0.09216]] tensor([[20.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7476  0.098   0.0706  0.08386]] tensor([[20.6875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.747   0.09796 0.07996 0.07513]] tensor([[21.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.741   0.0914  0.0972  0.07007]] tensor([[21.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6953  0.10016 0.11176 0.09265]] tensor([[21.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6646  0.07574 0.146   0.1137 ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.662  0.0778 0.1432 0.1169]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.691   0.0728  0.11816 0.11816]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.685   0.06573 0.1228  0.1267 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7     0.06934 0.1179  0.11255]] tensor([[19.7344]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6772  0.07367 0.1293  0.11957]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.674   0.07214 0.137   0.1171 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.642  0.0803 0.1388 0.1388]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6436 0.0871 0.121  0.1482]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.663   0.09705 0.1266  0.1134 ]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.685   0.09717 0.11725 0.1003 ]] tensor([[19.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6826  0.0984  0.12054 0.0984 ]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6816  0.10455 0.11847 0.0952 ]] tensor([[19.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.684   0.09705 0.11707 0.1017 ]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.6934 0.0868 0.115  0.1047]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7246  0.07635 0.0965  0.1027 ]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7217  0.0797  0.09467 0.10394]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.727  0.0778 0.0939 0.1015]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7344 0.0786 0.0905 0.0964]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.748   0.08527 0.08264 0.0839 ]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.751   0.097   0.07434 0.0779 ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.756  0.1123 0.065  0.0671]] tensor([[20.2812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7397  0.12463 0.0667  0.06885]] tensor([[20.4062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.724   0.12195 0.074   0.07996]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.721   0.11957 0.0784  0.08093]] tensor([[19.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7324  0.11774 0.0725  0.0772 ]] tensor([[20.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.735   0.1163  0.0716  0.07745]] tensor([[20.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7383  0.11676 0.07086 0.0742 ]] tensor([[19.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7485  0.1112  0.06744 0.07294]] tensor([[19.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7554  0.10547 0.07025 0.06915]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.731  0.1175 0.068  0.0833]] tensor([[20.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7256 0.1184 0.0696 0.0866]] tensor([[20.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.729   0.11005 0.06995 0.09125]] tensor([[20.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.766   0.0959  0.06192 0.07587]] tensor([[20.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7573 0.0978 0.0612 0.0837]] tensor([[20.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7524 0.0899 0.0689 0.0885]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.76    0.08527 0.0696  0.08527]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.758   0.08636 0.0694  0.08636]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.753   0.08856 0.07117 0.08716]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.745   0.09326 0.07263 0.089  ]] tensor([[19.5156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7407  0.09564 0.0722  0.0913 ]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7437  0.09607 0.07135 0.0888 ]] tensor([[19.7969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.746  0.0877 0.0786 0.0877]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.746   0.08374 0.08374 0.08636]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7383  0.0868  0.0841  0.09094]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.744   0.0848  0.0822  0.08887]] tensor([[19.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.73    0.1035  0.0781  0.08856]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7207  0.10547 0.0809  0.0931 ]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7466 0.0878 0.0763 0.0892]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7427  0.08875 0.0783  0.09015]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7407  0.0927  0.07684 0.08984]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7485 0.0894 0.0753 0.0867]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7446 0.0932 0.0749 0.0875]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.758   0.0774  0.0786  0.08636]] tensor([[19.3438]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.747   0.0878  0.08124 0.0838 ]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.747  0.0825 0.0865 0.0838]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.712   0.0877  0.10254 0.09784]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.701  0.0905 0.1042 0.1042]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7173 0.0792 0.105  0.0986]] tensor([[19.2656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7466  0.0663  0.09796 0.0892 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7373  0.0675  0.1014  0.09375]] tensor([[18.9062]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.723  0.0716 0.1042 0.101 ]] tensor([[18.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7397  0.06775 0.0986  0.09406]] tensor([[18.8125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7295 0.0831 0.0899 0.0972]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7246 0.0852 0.0921 0.0981]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.749   0.08673 0.0802  0.08405]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.767   0.07715 0.07715 0.07837]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7637 0.0792 0.078  0.0792]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7617  0.0766  0.08154 0.08026]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.768  0.0737 0.0797 0.0785]] tensor([[19.3594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.771   0.07513 0.07635 0.0775 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.771   0.07635 0.07513 0.0775 ]] tensor([[19.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.774  0.0766 0.0766 0.0731]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.769   0.0762  0.07733 0.07733]] tensor([[19.2969]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.771   0.0775  0.07513 0.07635]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.759   0.0775  0.07996 0.0838 ]] tensor([[19.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7607 0.0814 0.0777 0.0802]] tensor([[19.6562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.767   0.0796  0.076   0.07715]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.77    0.0811  0.07623 0.07275]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.761   0.08673 0.07654 0.0754 ]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7583  0.08777 0.07745 0.0763 ]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7344  0.10254 0.0891  0.07385]] tensor([[19.7500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.73    0.1068  0.08856 0.0746 ]] tensor([[19.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7407 0.1002 0.0871 0.0722]] tensor([[19.7812]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.772   0.088   0.0765  0.06335]] tensor([[19.7188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7603  0.0922  0.0789  0.06854]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7437  0.08746 0.0903  0.0784 ]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7563  0.0784  0.09174 0.0737 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.732   0.0874  0.10376 0.07715]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.723   0.0934  0.10254 0.0811 ]] tensor([[20.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.734   0.08105 0.10406 0.08105]] tensor([[19.6250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.728   0.0804  0.1099  0.08167]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.75    0.08026 0.0968  0.0731 ]] tensor([[19.4688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7534  0.0872  0.0872  0.07227]] tensor([[19.6406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.748   0.0839  0.0951  0.07294]] tensor([[19.4219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7197  0.0846  0.1212  0.07465]] tensor([[19.3281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7383  0.0841  0.10803 0.06976]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.738   0.08545 0.1047  0.07196]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.738   0.08405 0.09674 0.0815 ]] tensor([[18.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.776   0.07104 0.08435 0.06885]] tensor([[19.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.763   0.0792  0.0843  0.07324]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7676  0.08344 0.0809  0.0681 ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7686 0.0862 0.0773 0.0682]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.762   0.08826 0.0816  0.0677 ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7554  0.0931  0.08215 0.0692 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.76    0.08795 0.0839  0.0685 ]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7593 0.0921 0.0813 0.0674]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.764   0.08984 0.0793  0.0668 ]] tensor([[19.0625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7554  0.09454 0.07965 0.07025]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7607  0.0894  0.0802  0.06964]] tensor([[18.9688]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.766   0.08734 0.0783  0.068  ]] tensor([[19.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.756  0.0903 0.0835 0.0703]] tensor([[18.9219]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7637  0.09265 0.0756  0.0678 ]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7686 0.0918 0.0737 0.0661]] tensor([[19.1094]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.773  0.0909 0.073  0.0634]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7725  0.0982  0.0686  0.06052]] tensor([[19.2031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7764  0.09717 0.0668  0.05984]] tensor([[19.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.786   0.08954 0.0676  0.05692]] tensor([[19.0312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7744 0.0954 0.0687 0.0616]] tensor([[19.1562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7485 0.1045 0.0765 0.0707]] tensor([[19.4844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7188 0.1173 0.0858 0.0781]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7173  0.11896 0.0857  0.078  ]] tensor([[19.7656]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7295 0.1154 0.0831 0.0722]] tensor([[19.6719]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.75    0.0984  0.0816  0.06976]] tensor([[19.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.735   0.1059  0.0864  0.07275]] tensor([[19.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7417 0.0943 0.0819 0.0819]] tensor([[18.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.735   0.09344 0.0864  0.0851 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7363  0.09216 0.08655 0.0852 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.745  0.089  0.0876 0.0785]] tensor([[19.4375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7476  0.0907  0.0852  0.07635]] tensor([[19.5469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7476  0.0893  0.08655 0.0764 ]] tensor([[19.3125]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7466 0.0878 0.0892 0.0763]] tensor([[19.0156]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7485 0.0908 0.0867 0.0741]] tensor([[19.2500]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7485 0.088  0.088  0.0753]] tensor([[18.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7363  0.0996  0.08655 0.0776 ]] tensor([[18.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7446  0.0992  0.08484 0.0715 ]] tensor([[18.9531]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7456  0.1009  0.08105 0.07263]] tensor([[19.1250]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7344  0.10095 0.0837  0.0811 ]] tensor([[19.0938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.739   0.10156 0.0804  0.0791 ]] tensor([[19.3750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7554  0.09753 0.07477 0.07245]] tensor([[19.5781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7627 0.094  0.0755 0.0677]] tensor([[19.5938]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7637  0.09265 0.07684 0.0667 ]] tensor([[19.5625]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7637  0.0971  0.0756  0.06366]] tensor([[19.7031]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.756  0.1055 0.0748 0.064 ]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.76    0.10126 0.0764  0.06238]] tensor([[19.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.74    0.1083  0.08704 0.06464]] tensor([[19.9375]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7446  0.10565 0.0876  0.0621 ]] tensor([[19.8750]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7554  0.0991  0.0861  0.05917]] tensor([[19.8281]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7495  0.10144 0.08673 0.06247]] tensor([[19.8594]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7466  0.10425 0.08777 0.06128]] tensor([[19.8906]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.74    0.11    0.08435 0.0657 ]] tensor([[20.0469]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.731  0.1121 0.0887 0.068 ]] tensor([[19.9844]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.738  0.1114 0.0841 0.0665]] tensor([[20.]], device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7334 0.1142 0.0823 0.0704]] tensor([[20.1875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.728   0.11694 0.0842  0.0709 ]] tensor([[20.0781]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "[[0.7266  0.1186  0.0828  0.07196]] tensor([[20.2188]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "(224, 224, 3)\n",
      "File Ended\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Where is the danger?\"]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "vid_path = \"vids\"\n",
    "save_path = \"out\"\n",
    "count = 0\n",
    "refSub = None\n",
    "img_path = lambda a, b: a + \"\\\\\" + str(b) + \".jpg\" \n",
    "img = Image.open(img_path(vid_path, count))\n",
    "while(True):\n",
    "    image = preprocess(img).unsqueeze(0).to(device)\n",
    "    img_ret = interpret_vit(model=model, image=image, text=text, device=device, index=0, dangerVal = dangerMag(image))\n",
    "    print(img_ret.shape)\n",
    "    Image.fromarray(img_ret).save(img_path(save_path, count))\n",
    "    image = image.cpu()\n",
    "    img = None\n",
    "    try:\n",
    "        count+=1\n",
    "        img = Image.open(img_path(vid_path, count))\n",
    "    except:\n",
    "        print(\"File Ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8108cb5c-c660-4d6d-8d06-6cb3ef9c848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path_ret = 'ExplodeHeat4.mp4'\n",
    "img_paths = \"out\"\n",
    "count = 1\n",
    "\n",
    "img = cv2.imread(img_paths + \"\\\\\" + str(count) + \".jpg\" )\n",
    "height, width, layers = img.shape\n",
    "cap1 = cv2.VideoWriter(vid_path_ret, 0, 30, (OriginalSize[1], OriginalSize[0]))\n",
    "while(True):\n",
    "    imResiz = cv2.resize(img, (OriginalSize[1], OriginalSize[0]))\n",
    "    cap1.write(imResiz)\n",
    "    count += 2\n",
    "    img = cv2.imread(img_paths + \"\\\\\" + str(count) + \".jpg\" )\n",
    "    if(img is None):\n",
    "        break\n",
    "\n",
    "cap1.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df74631a-975d-4329-a312-36f28e53b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22.1406]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = Image.open(\"tempFol/0.jpg\")\n",
    "image = preprocess(img).unsqueeze(0).to(device)\n",
    "img_ret = interpret_vit(model=model, image=image, text=text, device=device, index=0)\n",
    "Image.fromarray(img_ret).save(\"tempFol/Heat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daba3763-f1c2-4c09-9550-e00fc74c2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Where is the danger?\"]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "vid_path = \"vids\"\n",
    "save_path = \"out\"\n",
    "count = 0\n",
    "refSub = None\n",
    "img_path = lambda a, b: a + \"\\\\\" + str(b) + \".jpg\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e08f24b-e5f9-4561-bd4d-a047a30af6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConstMemAll = torch.cuda.memory_allocated()\n",
    "ConstMemRes = torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8997121f-2117-4e3d-b042-8add27b7a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-4194304\n",
      "0\n",
      "-4194304\n",
      "0\n",
      "-4194304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.memory_allocated() - ConstMemAll)\n",
    "print(torch.cuda.memory_reserved() - ConstMemRes)\n",
    "\n",
    "#execute Clip VIL code\n",
    "#img_ret = interpret_vit(model=model, image=image, text=text, device=device, index=0)\n",
    "\n",
    "print(torch.cuda.memory_allocated() - ConstMemAll)\n",
    "print(torch.cuda.memory_reserved() - ConstMemRes)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated() - ConstMemAll)\n",
    "print(torch.cuda.memory_reserved() - ConstMemRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2c547c8-9640-4135-ad1c-ee2987a0ddfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 761959 KiB | 777830 KiB |   1802 MiB |   1058 MiB |\n",
      "|       from large pool | 702848 KiB | 715994 KiB |    699 MiB |     12 MiB |\n",
      "|       from small pool |  59111 KiB |  62553 KiB |   1103 MiB |   1045 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 761959 KiB | 777830 KiB |   1802 MiB |   1058 MiB |\n",
      "|       from large pool | 702848 KiB | 715994 KiB |    699 MiB |     12 MiB |\n",
      "|       from small pool |  59111 KiB |  62553 KiB |   1103 MiB |   1045 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 749488 KiB | 765359 KiB |   1790 MiB |   1058 MiB |\n",
      "|       from large pool | 690432 KiB | 703577 KiB |    687 MiB |     12 MiB |\n",
      "|       from small pool |  59056 KiB |  62493 KiB |   1103 MiB |   1045 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 776192 KiB |    772 MiB |    772 MiB |  14336 KiB |\n",
      "|       from large pool | 712704 KiB |    710 MiB |    710 MiB |  14336 KiB |\n",
      "|       from small pool |  63488 KiB |     62 MiB |     62 MiB |      0 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  14233 KiB |  28387 KiB |   1549 MiB |   1535 MiB |\n",
      "|       from large pool |   9856 KiB |  25600 KiB |    418 MiB |    409 MiB |\n",
      "|       from small pool |   4377 KiB |   4377 KiB |   1130 MiB |   1126 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1037    |    1077    |    3612    |    2575    |\n",
      "|       from large pool |     174    |     175    |     175    |       1    |\n",
      "|       from small pool |     863    |     903    |    3437    |    2574    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1037    |    1077    |    3612    |    2575    |\n",
      "|       from large pool |     174    |     175    |     175    |       1    |\n",
      "|       from small pool |     863    |     903    |    3437    |    2574    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      58    |      59    |      59    |       1    |\n",
      "|       from large pool |      27    |      28    |      28    |       1    |\n",
      "|       from small pool |      31    |      31    |      31    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      39    |      53    |    2230    |    2191    |\n",
      "|       from large pool |       2    |       6    |      28    |      26    |\n",
      "|       from small pool |      37    |      48    |    2202    |    2165    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00708e-fd2e-4e62-9fd0-91920510c640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
